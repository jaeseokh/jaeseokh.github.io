---
title: "Bayesian vs Frequentist"
author: "Jaeseok Hwang"
format:
  pdf:
    toc: true
    number-sections: true
    include-in-header:
      text: |
        \usepackage{amsmath, amssymb, bm}
        \usepackage{mathtools}
        \usepackage{siunitx}
        \DeclareMathOperator{\argmin}{arg\,min}
---

# eigenvalues/eigenvectors of Gram Matrix (X'X) tells everything about regression stability 

#  Eigenvalues = “how much spread/strength in each principal direction.”

# Eigenvectors = “which direction the ellipse axes point (rotation).” The tilt is controlled by correlation between Nitrogen and Precipitation.



# In learning graduate Econometrics and Statistics, Eigenvalue and Eigenvector are staying in the edge where student feel much irritated, dissapointed. Transforming the geometric concept that is normally used in physics and pure math into the idea for Economics and data analysis required a lot of stress for me. 

# For this reason, when I took Econometrics and Stat classes, I just skim the concep and memorize the definition and equation, but almost forgot what does it really means and how we can use it later. 

# As I review my models in disseration chapter, I found some scale and collinearity problem in my regression based machine learning model, and tried to investigate so deep, and I found that it is time to fight with eigenvector and eigenvalue which has been blocking my deep understanding about huge part of feature scalining in machine learning and neural netwark models. 

# So, here I want to make my own note to remind myself about how I can perceive eigenvalue and eigenvector. 

# Okay, let's go!

# In geometry and physics
# what is eigenvector simply , it is about 

# Simplifying Transformations:
# Eigenvectors reveal the essential behavior of a linear transformation. When a matrix (which represents a transformation) acts on an eigenvector, the result is simply the original eigenvector multiplied by a scalar (the eigenvalue). This means the eigenvector's direction is preserved

# Differential Equations:

# Eigenvectors are used to find solutions to systems of differential equations. They allow for the decomposition of complex solutions into simpler components. 

#  "eigenvectors are the axes of the ellipse, eigenvalues are the squared lengths of those axes."




# 1. OLS and the Normal Equations

Consider a regression with two predictors \(x_1=\) Nitrogen and \(x_2=\) Precipitation. Let \(X\in\mathbb{R}^{n\times 2}\) be the design matrix (columns \(x_1,x_2\)), and \(y\in\mathbb{R}^n\) the response (e.g., yield).

The ordinary least squares (OLS) estimator solves
$$
\hat{\beta}
= \argmin_{\beta\in\mathbb{R}^2}\ \|y - X\beta\|_2^2
\quad\Rightarrow\quad
\hat{\beta} = (X'X)^{-1}X'y,
$$
where \(X'X\) is the \(2\times 2\) Gram matrix.

# 2. Structure of the Gram Matrix \(X'X\)

With two predictors,
$$
X'X =
\begin{bmatrix}
\sum_i x_{i1}^2 & \sum_i x_{i1}x_{i2} \\
\sum_i x_{i1}x_{i2} & \sum_i x_{i2}^2
\end{bmatrix}
= 
\begin{bmatrix}
a & c\\
c & b
\end{bmatrix},
$$
where \(a=\sum_i x_{i1}^2\) (scale of Nitrogen), \(b=\sum_i x_{i2}^2\) (scale of Precipitation), and \(c=\sum_i x_{i1}x_{i2}\) (covariance-like cross term).

The associated quadratic form for a generic vector \(x=(x_1,x_2)'\) is
$$
x'(X'X)x = a x_1^2 + 2c x_1x_2 + b x_2^2.
$$
Here, \(a,b\) control the axis lengths (scale), while \(c\neq 0\) introduces **tilt** (rotation) via the cross term \(2c x_1x_2\).

# 3. From Quadratic Form to an Ellipse

When \(X'X\) is symmetric positive definite, the level set
$$
\{x\in\mathbb{R}^2: x'(X'X)x = 1\}
$$
is an ellipse in the \((x_1,x_2)\)-plane. If \(c=0\) the ellipse is axis-aligned. If \(c\neq 0\), the ellipse is tilted.

Diagonal entries \((a,b)\) reflect the variances (scales) of each predictor; the off-diagonal \(c\) controls the degree of tilt through the cross term.

# 4. Eigen-Decomposition and Coordinate Rotation

Let the eigendecomposition of \(X'X\) be
$$
X'X = Q\,\Lambda\,Q',
\quad
Q=\begin{bmatrix} \vert & \vert \\ v_1 & v_2 \\ \vert & \vert \end{bmatrix},
\quad
\Lambda=\mathrm{diag}(\lambda_1,\lambda_2),
$$
with orthonormal eigenvectors \(v_1,v_2\) and eigenvalues \(\lambda_1,\lambda_2>0\).

Define the rotated coordinates \(y = Q' x\). Then
$$
x'(X'X)x
= x'Q\Lambda Q' x
= y' \Lambda y
= \lambda_1 y_1^2 + \lambda_2 y_2^2,
$$
so in the eigen-basis the ellipse becomes **axis-aligned** (no cross term). Geometrically:
- **Eigenvectors \(v_j\)** give the principal directions (rotation directions) of the ellipse.
- **Eigenvalues \(\lambda_j\)** give the strength of spread (information) along those directions.
- Axis lengths are proportional to \(1/\sqrt{\lambda_j}\).

# 5. Inversion and OLS Coefficients in the Eigen-Basis

The inverse of \(X'X\) follows from the eigendecomposition:
$$
(X'X)^{-1} = Q\,\Lambda^{-1}\,Q',
\qquad
\Lambda^{-1}=\mathrm{diag}\!\left(\tfrac{1}{\lambda_1},\tfrac{1}{\lambda_2}\right).
$$
Therefore, the OLS estimator can be written as
$$
\hat{\beta}
= (X'X)^{-1}X'y
= Q\,\Lambda^{-1}\,Q'\,X'y.
$$

**Operationally this means:**
1. **Rotate** the predictor space into independent directions: \(u = Q'X'y\).
2. **Scale** each direction by the inverse eigenvalues: \(u^* = \Lambda^{-1} u\).
3. **Rotate back** to the original Nitrogen/Precipitation basis: \(\hat{\beta} = Q u^*\).

**Stability implication:** if an eigenvalue \(\lambda_j\) is small, its reciprocal \(1/\lambda_j\) is large, magnifying noise and making \(\hat{\beta}\) unstable in that direction (classic ill-conditioning).

# 6. Closed-Form 2×2 Eigenpairs and Rotation Angle

For \(X'X=\begin{bmatrix} a & c\\ c & b\end{bmatrix}\), the eigenvalues are
$$
\lambda_{\pm}
= \frac{(a+b)\ \pm\ \sqrt{(a-b)^2 + 4c^2}}{2}.
$$
A corresponding (unnormalized) eigenvector for \(\lambda\) can be taken from \((X'X-\lambda I)v=0\), e.g.,
$$
v(\lambda) \propto \begin{bmatrix} -c \\ a-\lambda \end{bmatrix}
\quad\text{or}\quad
\begin{bmatrix} b-\lambda \\ -c \end{bmatrix}.
$$
The **rotation angle** \(\theta\) of the principal axis relative to the \(x_1\)-axis satisfies
$$
\tan(2\theta) = \frac{2c}{a-b}.
$$
- If \(c=0\), then \(\theta=0\): axes aligned with \(x_1,x_2\).
- Larger \(|c|\) means a greater tilt of the ellipse.

# 7. Iterative Eigen Methods (Link to Information Directions)

Two standard iterative methods illustrate how the eigen-directions emerge from the data geometry:

**Power iteration (largest eigenpair):**
$$
\text{Initialize } v^{(0)}\neq 0,\quad
v^{(k+1)} = \frac{(X'X)\,v^{(k)}}{\|(X'X)\,v^{(k)}\|},
\quad
\text{Rayleigh quotient } \rho^{(k)}=\frac{{v^{(k)}}' (X'X) v^{(k)}}{{v^{(k)}}'v^{(k)}},
$$
converges to \((\lambda_{\max}, v_{\max})\).

**Inverse iteration (smallest eigenpair, shift \(=0\)):**
$$
\text{Initialize } w^{(0)}\neq 0,\quad
\tilde{w}^{(k+1)} = (X'X)^{-1} w^{(k)}\quad(\text{solve }(X'X)\tilde{w}^{(k+1)}=w^{(k)}),
\quad
w^{(k+1)} = \frac{\tilde{w}^{(k+1)}}{\|\tilde{w}^{(k+1)}\|},
$$
with Rayleigh quotient \(\frac{{w^{(k)}}' (X'X) w^{(k)}}{{w^{(k)}}'w^{(k)}}\) converging to \(\lambda_{\min}\).

These procedures reveal that **eigenvectors are the natural (most/least informative) directions** of the Gram matrix; OLS inverts \(X'X\) and thus inherently weights these directions by \(1/\lambda_j\).

---

## (Optional) Appendix: Concrete Numeric Sketch

Let
$$
X'X = \begin{bmatrix} a & c\\ c & b\end{bmatrix}
= \begin{bmatrix} 900 & 120\\ 120 & 400\end{bmatrix},
\qquad
X'y = \begin{bmatrix} d\\ e\end{bmatrix}
= \begin{bmatrix} 5000\\ 3000\end{bmatrix}.
$$
1) Compute \(\lambda_{\pm}\) via the closed form above.  
2) Get \(Q=[v_+,v_-]\) by normalizing eigenvectors.  
3) Form \(\Lambda^{-1}=\mathrm{diag}(1/\lambda_+,1/\lambda_-)\).  
4) Compute \(u=Q'X'y\), then \(u^*=\Lambda^{-1}u\), and finally \(\hat{\beta}=Q u^*\).

This yields the same \(\hat{\beta}\) as directly inverting \(X'X\) (up to numerical rounding), but makes explicit **how** eigenvalues/eigenvectors control the stability and geometry of the solution.
